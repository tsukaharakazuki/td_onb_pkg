# TD Article Tags Workflow

このWorkflowは、メディア・EC向けに、Webページコンテンツの全文の本文データを形態素解析にて単語抽出し、ユーザーのWeb閲覧データと組み合わせて、ユーザーごとに関心ワードを付与します。

## 処理概要

このワークフローは以下の処理を行います：

1. **ターゲット記事データの準備** (`+prep` > `+target`)
   - 設定ファイルで指定された記事データベースから記事ID、記事テキスト（タイトルや本文）を抽出

2. **メイン処理** (`+main`)
   - **形態素解析** (`+tokenize_ja`): Hivemallの`tokenize_ja`を使用して記事テキストを単語に分解
   - **TF-IDF計算** (`+tf_idf`): 単語の重要度をTF-IDF値で計算
   - **記事タグ生成** (`+dist`): 各記事IDに対して重要単語（タグ）を集約

3. **オリジナル関心ワード生成** (`+original_interest_words`) - 条件付き実行
   - **アクセスログ処理** (`+access_log`): ユーザーの記事閲覧履歴を集計
   - **関心ワードベース生成** (`+interest_words_base`): 閲覧履歴と記事の重要語を組み合わせてスコア計算
   - **ユーザー関心ワード生成** (`+dist`): ユーザーごとの関心ワードを集約

## 設定方法

設定は以下のファイルで行います：

- `config/params.yml`: 基本パラメータ（ターゲットDB/テーブル、テキストカラム、トークン化設定など）
- `config/stopwords.yml`: ストップワード（除外する単語）の定義
- `config/error.dig`: エラー通知設定

主な設定項目：
- ターゲットデータ: 記事DB、テーブル、キーカラム、テキストカラム
- 形態素解析: `tokenize_ja`の設定（テキスト正規化、除外する品詞、ストップワード）
- 関心ワード生成: Webログデータ、ユーザーID、閲覧期間など

## Hivemall

この処理内ではHivemallの`tokenize_ja`が利用されています。この関数は日本語テキストを形態素解析し、品詞情報とともに単語に分解します。

形態素解析によって、以下のような品詞が識別されます：
- 名詞（一般、固有名詞、サ変接続など）
- 動詞（自立、非自立など）
- 形容詞
- 副詞
- 連体詞
- など

デフォルト設定では、「名詞-一般」と「名詞-固有名詞」の単語のみを抽出します。

### 参考ドキュメント
- [Hivemall tokenize_ja の使い方](https://qiita.com/myui/items/54f7fcce6bdeacd9674a)
- 詳細な品詞タグ情報は `docs/tokenize_ja.md` を参照
